{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 80\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT=0.2 # my set\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "texts_raw=[]\n",
    "f1=open(\"D:\\\\data\\\\wangli\\\\pos_fenci.txt\",errors=\"ignore\")\n",
    "lines1=f1.readlines();\n",
    "for line in lines1:\n",
    "    _item=line.strip();\n",
    "    texts.append(_item);\n",
    "    texts_raw.append(_item)\n",
    "    labels.append(1);\n",
    "    \n",
    "f2=open(\"d:\\\\data\\\\wangli\\\\neg_fenci.txt\",errors=\"ignore\")\n",
    "lines2=f2.readlines();\n",
    "for line in lines2:\n",
    "    _item=line.strip();\n",
    "    texts.append(_item);\n",
    "    texts_raw.append(_item)\n",
    "    labels.append(0);\n",
    "    \n",
    "f3=open(\"d:\\\\data\\\\wangli\\\\dm_fenci.txt\",errors='ignore')\n",
    "lines3=f3.readlines()\n",
    "for line in lines3:\n",
    "    _item=line.strip();\n",
    "    texts_raw.append(_item)\n",
    "    \n",
    "f4=open(\"d:\\\\data\\\\wangli\\\\rw_fenci.txt\",errors='ignore')\n",
    "lines4=f4.readlines()\n",
    "for line in lines4:\n",
    "    _item=line.strip()\n",
    "    texts_raw.append(_item)\n",
    "    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\preprocessing\\text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79598 unique tokens.\n",
      "Shape of data tensor: (21124, 80)\n",
      "Shape of label tensor: (21124, 2)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_raw)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.6 * data.shape[0])\n",
    "\n",
    "\n",
    "x_train = data[:nb_validation_samples]\n",
    "y_train = labels[:nb_validation_samples]\n",
    "\n",
    "x_val = data[nb_validation_samples:nb_validation_samples+int(0.2*data.shape[0])]\n",
    "y_val = labels[nb_validation_samples:nb_validation_samples+int(0.2*data.shape[0])]\n",
    "\n",
    "x_test=data[nb_validation_samples+int(0.2*data.shape[0]):]\n",
    "y_test=labels[nb_validation_samples+int(0.2*data.shape[0]):]\n",
    "\n",
    "\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 98625 word vectors in Glove 6B 100d.\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"D:\\\\data\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'vector.txt'),encoding='utf-8',errors=\"ignore\")\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1188: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "#model.add(Flatten())\n",
    "model.add(layers.LSTM(128))#,return_sequences=True)\n",
    "model.add(layers.Dropout(0.5));\n",
    "#model.add(layers.LSTM(128))\n",
    "#model.add(layers.Dropout(0.5));\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 12674 samples, validate on 4224 samples\n",
      "Epoch 1/20\n",
      "12674/12674 [==============================] - 8s - loss: 0.4735 - acc: 0.7832 - val_loss: 0.3434 - val_acc: 0.8653\n",
      "Epoch 2/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.3390 - acc: 0.8663 - val_loss: 0.4144 - val_acc: 0.8163\n",
      "Epoch 3/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.2856 - acc: 0.8874 - val_loss: 0.2756 - val_acc: 0.8951\n",
      "Epoch 4/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.2481 - acc: 0.9020 - val_loss: 0.3686 - val_acc: 0.8504\n",
      "Epoch 5/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.2132 - acc: 0.9188 - val_loss: 0.2508 - val_acc: 0.9060\n",
      "Epoch 6/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.1834 - acc: 0.9329 - val_loss: 0.2310 - val_acc: 0.9129\n",
      "Epoch 7/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.1510 - acc: 0.9450 - val_loss: 0.2501 - val_acc: 0.9115\n",
      "Epoch 8/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.1254 - acc: 0.9553 - val_loss: 0.2682 - val_acc: 0.8987\n",
      "Epoch 9/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.1000 - acc: 0.9650 - val_loss: 0.2583 - val_acc: 0.9176\n",
      "Epoch 10/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0813 - acc: 0.9725 - val_loss: 0.2497 - val_acc: 0.9183\n",
      "Epoch 11/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0664 - acc: 0.9779 - val_loss: 0.2829 - val_acc: 0.9141\n",
      "Epoch 12/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0504 - acc: 0.9829 - val_loss: 0.5393 - val_acc: 0.8776\n",
      "Epoch 13/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0423 - acc: 0.9851 - val_loss: 0.3185 - val_acc: 0.9214\n",
      "Epoch 14/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0355 - acc: 0.9882 - val_loss: 0.3193 - val_acc: 0.9240\n",
      "Epoch 15/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0246 - acc: 0.9927 - val_loss: 0.3518 - val_acc: 0.9214\n",
      "Epoch 16/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0219 - acc: 0.9931 - val_loss: 0.3672 - val_acc: 0.9214\n",
      "Epoch 17/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0178 - acc: 0.9944 - val_loss: 0.5385 - val_acc: 0.8859\n",
      "Epoch 18/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0192 - acc: 0.9949 - val_loss: 0.3835 - val_acc: 0.9242\n",
      "Epoch 19/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0152 - acc: 0.9948 - val_loss: 0.4291 - val_acc: 0.9179\n",
      "Epoch 20/20\n",
      "12674/12674 [==============================] - 7s - loss: 0.0143 - acc: 0.9955 - val_loss: 0.4142 - val_acc: 0.9238\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是分析和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "results=model.predict(x_test)\n",
    "from numpy import argmax\n",
    "y_pre = argmax(results,axis=1) \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pre[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9135638741702199\n",
      "0.9137066791193937\n",
      "0.9136081949137216\n",
      "以下是IMRD部分\n",
      "0\n",
      "0.907221425155428\n",
      "0.5\n",
      "0.453610712577714\n",
      "0.47567703109327986\n",
      "1\n",
      "0.9199063231850118\n",
      "0.5\n",
      "0.4599531615925059\n",
      "0.47914125396438156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\qct20\\Anaconda2\\envs\\gpu_env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score;\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score;\n",
    "lst1=[]#true,in fact\n",
    "for i in range(0,len(y_test)):\n",
    "   \n",
    "    if y_test[i][0]==0 and y_test[i][1]==1:\n",
    "        lst1.append(1);\n",
    "    if y_test[i][0]==1 and y_test[i][1]==0:\n",
    "        lst1.append(0);\n",
    "   \n",
    "yy_test=np.array(lst1)\n",
    "p=precision_score(y_pre,yy_test,average='macro')\n",
    "r=recall_score(y_pre,yy_test,average='macro')\n",
    "f1=f1_score(y_pre,yy_test,average='macro')\n",
    "print(p)\n",
    "print(r)\n",
    "print(f1)\n",
    "\n",
    "print(\"以下是IMRD部分\")\n",
    "dic=dict()\n",
    "for i in range(0,len(yy_test)):\n",
    "    if yy_test[i] not in dic:\n",
    "        lst_small=list();\n",
    "        lst_small.append(y_pre[i])\n",
    "        dic[yy_test[i]]=lst_small;\n",
    "    else:\n",
    "        dic[yy_test[i]].append(y_pre[i])\n",
    "dic=dict(sorted(dic.items(),key=lambda x:x[0]))\n",
    "\n",
    "for ee in dic:\n",
    "    lst_sub=list();\n",
    "    for j in range(0,len(dic[ee])):\n",
    "        lst_sub.append(ee)\n",
    "    print(ee)\n",
    "    \n",
    "    print(accuracy_score(np.array(lst_sub),np.array(dic[ee])))\n",
    "    p=precision_score(np.array(lst_sub),np.array(dic[ee]),average='macro');\n",
    "    r=recall_score(np.array(lst_sub),np.array(dic[ee]),average='macro');\n",
    "    f1=f1_score(np.array(lst_sub),np.array(dic[ee]),average='macro');\n",
    "   \n",
    "    print(p)\n",
    "    print(r)\n",
    "    print (f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
